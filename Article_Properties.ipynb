{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The purpose of this script is to collect basic properties of an article.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipedia as wiki\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Article\n",
    "lang= 'en'\n",
    "title = 'World War II'\n",
    "wikiID = \"32927\" #from Page info of article, e.g. https://en.wikipedia.org/w/index.php?title=World_War_II&action=info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i+n]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(request):\n",
    "    request['action'] = 'query'\n",
    "    request['format'] = 'json'\n",
    "    lastContinue = {}\n",
    "    while True:\n",
    "        req = request.copy()\n",
    "        req.update(lastContinue)\n",
    "        result = requests.get('https://%s.wikipedia.org/w/api.php'%lang, params=req).json()\n",
    "        if 'error' in result:\n",
    "            raise Error(result['error'])\n",
    "        if 'warnings' in result:\n",
    "            print(result['warnings'])\n",
    "        if 'query' in result:\n",
    "            yield result['query']\n",
    "        if 'continue' not in result:\n",
    "            break\n",
    "        lastContinue = result['continue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET WIKIDATA ID\n",
    "for result in query({'titles': title, 'prop': 'pageprops'}):\n",
    "    ID = result\n",
    "    ID = ID['pages'][wikiID]['pageprops']['wikibase_item']\n",
    "    print(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PageLength\n",
    "for result in query({'titles': title, 'prop': 'info'}):\n",
    "    LENGTH = result\n",
    "    LENGTH = LENGTH['pages'][wikiID]['length']\n",
    "    print('The article has',LENGTH,'words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET INTERNAL OUT LINKS\n",
    "links_list = []\n",
    "\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'links', 'pllimit':'max','plnamespace':'0', 'redirects':''})),50):\n",
    "    LINKS = result\n",
    "    for l in LINKS:\n",
    "        links = l['pages'][wikiID]['links']\n",
    "        for m in links:\n",
    "            n = m['title']\n",
    "            links_list.append(n)\n",
    "\n",
    "links_df = pd.DataFrame(links_list, columns=['Article'])\n",
    "print('There are ',len(links_df.index),'links.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET EXTERNAL OUT LINKS\n",
    "exlinks_list = []\n",
    "\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'extlinks', 'ellimit':'max'})),50):\n",
    "    EXLINKS = result\n",
    "    for x in EXLINKS:\n",
    "        exlinks = x['pages'][wikiID]['extlinks']\n",
    "        for extl in exlinks:\n",
    "            exlinks = extl['*']\n",
    "            exlinks_list.append(exlinks)\n",
    "\n",
    "exlinks_df = pd.DataFrame(exlinks_list, columns=['Ext Link'])\n",
    "print('There are ',len(exlinks_df.index),'external links.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET COUNT OF LANGUAGE LINKS\n",
    "llinks_list = []\n",
    "\n",
    "for result in query({'titles': title, 'prop': 'langlinks', 'lllimit':'max'}):\n",
    "    LLINKS = result\n",
    "    llinks = LLINKS['pages'][wikiID]['langlinks']\n",
    "    for ll in llinks:\n",
    "        llinks = ll['lang']\n",
    "        llinks_list.append(llinks)\n",
    "        \n",
    "llinks_df = pd.DataFrame(llinks_list, columns=['Lang Link'])\n",
    "print(len(llinks_df.index),'other language editions have articles about this concept.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGE\n",
    "for result in chunks(list(query({'titles':title, 'prop': 'revisions', 'rvprop':'timestamp','rvlimit':'max'})),50):\n",
    "    TIMESTAMPS = result\n",
    "    for r in TIMESTAMPS:\n",
    "        s = r['pages'][wikiID]['revisions']\n",
    "        for t in s:\n",
    "            first = t['timestamp']\n",
    "            dt = datetime.strptime(first, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "            now = datetime.now()\n",
    "            delta = now - dt.replace(tzinfo=None)\n",
    "            years = delta.days/365\n",
    "\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGES\n",
    "images_list = []\n",
    "\n",
    "for result in query({'titles': title, 'prop': 'images', 'imlimit':'max'}):\n",
    "    IMAGES = result\n",
    "    images = IMAGES['pages'][wikiID]['images']\n",
    "    for i in images:\n",
    "        images = i['title']\n",
    "        images_list.append(images)\n",
    "\n",
    "images_df = pd.DataFrame(images_list, columns=['Images'])\n",
    "print('There are',len(images_df.index),'images in this article.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITORS   \n",
    "editor_list = []\n",
    "\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'contributors', 'pclimit':'max'})),50):\n",
    "    EDITORS = result\n",
    "    for e in EDITORS:\n",
    "        editors = e['pages'][wikiID]['contributors']\n",
    "        anoneds = e['pages'][wikiID]['anoncontributors']\n",
    "        for ed in editors:\n",
    "            eds = ed['name']\n",
    "            editor_list.append(eds)\n",
    "\n",
    "editor_df = pd.DataFrame(editor_list, columns=['Username'])\n",
    "edscount = len(editor_df)\n",
    "print('There are ',edscount,'editors.\\n')\n",
    "print('There are', anoneds,'anonymous editors.\\n')\n",
    "totaleds = anoneds+edscount\n",
    "print('In total,', totaleds, 'editors have edited this article.\\n')\n",
    "propanon = anoneds/totaleds\n",
    "print('Proportion of anonymous editors:', propanon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count edits per editor\n",
    "rev_list = []\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'revisions', 'rvprop':'user','rvlimit':'max'})),50):\n",
    "    REVISIONS = result\n",
    "    for r in REVISIONS:\n",
    "        revs = r['pages'][wikiID]['revisions']\n",
    "        for ed in revs:\n",
    "            eds = ed['user']\n",
    "            rev_list.append(eds)\n",
    "\n",
    "rev_df = pd.DataFrame(rev_list, columns=['Editor'])\n",
    "totaledits = len(rev_df.index)\n",
    "meanedits = round(totaledits/totaleds)\n",
    "\n",
    "print('This article has been edited',len(rev_df.index),'times.\\n')\n",
    "print('On average, this article was edited', meanedits,'times per editor.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET SUMMARY STATISTICS ABOUT EDITORS\n",
    "eds_vc = rev_df['Editor'].value_counts().sort_values(ascending=False)\n",
    "eds_vc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW MOST ACTIVE EDITORS\n",
    "display(eds_vc.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISE DISTRIBUTION OF EDITS PER EDITOR\n",
    "sns.set()\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(eds_vc, kde=False, rug=False).set_title('Edits per Editor')\n",
    "plt.ylabel(\"Count of Editors\")\n",
    "plt.xlabel(\"Count of Edits\")\n",
    "plt.tight_layout() \n",
    "#ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "plt.savefig('editspereditor.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRODUCE EDIT HISTORY DATAFRAME\n",
    "editZ = []\n",
    "        \n",
    "# PRODUCE DATAFRAME OF EDITS PER YEAR\n",
    "for i in rev_list:\n",
    "    for k,v in i.items():\n",
    "        v = datetime.strptime(v, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        v = v.strftime('%Y')\n",
    "        editZ.append(v)\n",
    "        \n",
    "df = pd.Series(editZ).value_counts()\n",
    "\n",
    "# PRODUCE CUMULATIVE VALUES\n",
    "df.index = df.index.astype(int)\n",
    "df = df.fillna(0)\n",
    "df = df.to_frame()\n",
    "df.index.name = 'Date'\n",
    "df.columns = ['Edits EN']\n",
    "df = df.sort_values(by=['Date'])\n",
    "df['Cum EN'] = df['Edits EN'].cumsum(axis = 0)\n",
    "\n",
    "# NORMALISE DATA\n",
    "counts =  df['Edits EN'].sum()\n",
    "df_norm = df / counts\n",
    "\n",
    "# NOW WE HAVE 2 DATAFRAMES OF EDITS PER YEAR: df AND df_norm\n",
    "display(df)\n",
    "#display(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S COMPARE ARTICLES IN 2 LANGUAGE EDITIONS\n",
    "# LET'S COMPARE (1) EDIT HISTORIES & (2) HYPERLINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German WW2 Article:\n",
    "lang = 'de'\n",
    "title = 'Zweiter Weltkrieg'\n",
    "wikiID = '5767'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(request):\n",
    "    request['action'] = 'query'\n",
    "    request['format'] = 'json'\n",
    "    lastContinue = {}\n",
    "    while True:\n",
    "        req = request.copy()\n",
    "        req.update(lastContinue)\n",
    "        result = requests.get('https://%s.wikipedia.org/w/api.php'%lang, params=req).json()\n",
    "        if 'error' in result:\n",
    "            raise Error(result['error'])\n",
    "        if 'warnings' in result:\n",
    "            print(result['warnings'])\n",
    "        if 'query' in result:\n",
    "            yield result['query']\n",
    "        if 'continue' not in result:\n",
    "            break\n",
    "        lastContinue = result['continue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET EDIT HISTORY OF SECOND ARTICLE AND CALL IT rev_list2\n",
    "rev_list2 = []\n",
    "editZ2 = []\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'revisions', 'rvprop':'timestamp','rvlimit':'max'})),50):\n",
    "    REVISIONS = result\n",
    "    for r in REVISIONS:\n",
    "        revs = r['pages'][wikiID2]['revisions']\n",
    "        for t in revs:\n",
    "            rev_list2.append(t) \n",
    "\n",
    "# PRODUCE DATAFRAME OF EDITS PER YEAR\n",
    "for i in rev_list2:\n",
    "    for k,v in i.items():\n",
    "        v = datetime.strptime(v, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        v = v.strftime('%Y')\n",
    "        editZ2.append(v)\n",
    "df2 = pd.Series(editZ2).value_counts()\n",
    "\n",
    "# PRODUCE CUMULATIVE VALUES\n",
    "df2.index = df2.index.astype(int)\n",
    "df2 = df2.fillna(0)\n",
    "df2 = df2.to_frame()\n",
    "df2.index.name = 'Date'\n",
    "df2.columns = ['Edits DE']\n",
    "df2 = df2.sort_values(by=['Date'])\n",
    "df2['Cum DE'] = df2['Edits DE'].cumsum(axis = 0)\n",
    "\n",
    "# NORMALISE DATA\n",
    "counts2 =  df2['Edits DE'].sum()\n",
    "df2_norm = df2 / counts2\n",
    "\n",
    "# NOW WE HAVE 2 DATAFRAMES OF EDITS PER YEAR: df AND df_norm\n",
    "display(df2)\n",
    "#display(df2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE NORMALISED EDIT HISTORIES\n",
    "edit_df = pd.merge(df_norm, df2_norm, how='inner',on='Date',left_index=True)\n",
    "edit_df = edit_df.drop(columns=['Cum EN','Cum DE'])\n",
    "\n",
    "cum_df = pd.merge(df_norm, df2_norm, how='inner',on='Date',left_index=True)\n",
    "cum_df = cum_df.drop(columns=['Edits EN','Edits DE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISE EDIT HISTORIES\n",
    "sns.set()\n",
    "\n",
    "edit_df.plot()\n",
    "ax.figure.legend()\n",
    "plt.xlabel('Date')\n",
    "ax.xaxis.set_ticks(np.arange(2001, 2020))\n",
    "plt.locator_params(axis='x', nbins=15)\n",
    "plt.ylabel('Proportion of Edits')\n",
    "plt.title('Edit History of Articles')\n",
    "plt.savefig('edit_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISE CUMULATIVE EDIT HISTORIES\n",
    "cum_df.plot()\n",
    "ax.figure.legend()\n",
    "plt.xlabel('Date')\n",
    "ax.xaxis.set_ticks(np.arange(2001, 2020))\n",
    "plt.locator_params(axis='x', nbins=15)\n",
    "plt.ylabel('Proportion of Edits')\n",
    "plt.title('Cumulative Edit History of Articles')\n",
    "plt.savefig('cum_edithistory.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that we have compared the edit histories, let's compare the link similarity of the articles.\n",
    "To do this, we need to:\n",
    "(1) collect the hyperlinks\n",
    "(2) collect WikiData IDs for hyperlinks\n",
    "(3) compute cosine similarity of IDs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First reintroduce article specification:\n",
    "\n",
    "#Specify Article\n",
    "lang= 'en'\n",
    "title = 'World War II'\n",
    "wikiID = \"32927\" #from Page info of article, e.g. https://en.wikipedia.org/w/index.php?title=World_War_II&action=info\n",
    "\n",
    "#GET LINKS in LANG1\n",
    "links_list = []\n",
    "\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'links', 'pllimit':'max','plnamespace':'0', 'redirects':''})),50):\n",
    "    LINKS = result\n",
    "    for l in LINKS:\n",
    "        links = l['pages'][wikiID]['links']\n",
    "        for m in links:\n",
    "            n = m['title']\n",
    "            links_list.append(n)\n",
    "\n",
    "links_df = pd.DataFrame(links_list, columns=['Article'])\n",
    "\n",
    "#GET IDS OF LINKS\n",
    "ids = {}\n",
    "for i in chunks(links_list, 50):\n",
    "    for result in query({'titles':'|'.join(i), 'prop':'pageprops','ppprop':'wikibase_item'}):\n",
    "        for j in result['pages'].values():\n",
    "            if 'pageprops' in j.keys():\n",
    "                ids[j['title']] = j['pageprops']['wikibase_item']\n",
    "                \n",
    "#Make Dataframe of Articles and IDs\n",
    "ids_df = pd.DataFrame.from_dict(ids,orient='index', columns=['WikiData ID'])\n",
    "ids_df = ids_df.reset_index()\n",
    "ids_df.columns = [lang, 'WikiData ID']\n",
    "display(ids_df.head())\n",
    "\n",
    "#Get link summaries\n",
    "ids = len(ids_df.index)\n",
    "links = len(links_df.index)\n",
    "print('In total there are', links,'links in the', lang ,'article. Of these,', ids, 'are unique links.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now specify German article again\n",
    "lang = 'de'\n",
    "title = 'Zweiter Weltkrieg'\n",
    "wikiID = '5767'\n",
    "\n",
    "links_list2 = []\n",
    "\n",
    "for result in chunks(list(query({'titles': title, 'prop': 'links', 'pllimit':'max','plnamespace':'0', 'redirects':''})),50):\n",
    "    LINKS = result\n",
    "    for l in LINKS:\n",
    "        links = l['pages'][wikiID2]['links']\n",
    "        for m in links:\n",
    "            n = m['title']\n",
    "            links_list2.append(n)\n",
    "\n",
    "links_df2 = pd.DataFrame(links_list2, columns=['Article'])\n",
    "\n",
    "#GET IDS OF LINKS\n",
    "Articles2 = list(links_df2['Article'])\n",
    "\n",
    "ids2 = {}\n",
    "for i in chunks(Articles2, 50):\n",
    "    for result in query({'titles':'|'.join(i), 'prop':'pageprops','ppprop':'wikibase_item'}):\n",
    "        for j in result['pages'].values():\n",
    "            if 'pageprops' in j.keys():\n",
    "                ids2[j['title']] = j['pageprops']['wikibase_item']\n",
    "\n",
    "#Make Dataframe of Articles and IDs\n",
    "ids_df2 = pd.DataFrame.from_dict(ids2,orient='index', columns=['WikiData ID'])\n",
    "ids_df2 = ids_df2.reset_index()\n",
    "ids_df2.columns = [lang2, 'WikiData ID']\n",
    "display(ids_df2.head())\n",
    "\n",
    "#Get link summaries\n",
    "ids2 = len(ids_df2.index)\n",
    "links2 = len(links_df2.index)\n",
    "print('In total there are', links2,'links in the',lang2,'article. Of these,', ids2, 'are unique links.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Dataframes and get link overlap\n",
    "mergeddf = pd.merge(ids_df, ids_df2, on='WikiData ID')\n",
    "mergeddf = mergeddf.set_index(['WikiData ID'])\n",
    "common = len(mergeddf)\n",
    "display(mergeddf.head())\n",
    "unmerged = ids_df[~ids_df.isin(mergeddf)].dropna()\n",
    "unmerged2 = ids_df[~ids_df2.isin(mergeddf)].dropna()\n",
    "\n",
    "overlap = \"{0:.00%}\".format(common / ids)\n",
    "overlap2 = \"{0:.00%}\".format(common / ids2)\n",
    "\n",
    "print(overlap,'of unique links in the EN article overlap with unqiue links in the DE article.')\n",
    "print(overlap2,'of unique links in the DE article overlap with unqiue links in the EN article.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Dataframe for Cosine Similarity\n",
    "\n",
    "EN = ids_df.copy()\n",
    "DE = ids_df2.copy()\n",
    "EN['In_DE'] = EN['WikiData ID'].isin(DE['WikiData ID'])\n",
    "DE['In_EN'] = DE['WikiData ID'].isin(EN['WikiData ID'])\n",
    "EN = EN.drop(columns =['en'])\n",
    "DE = DE.drop(columns =['de'])\n",
    "\n",
    "#Combine Dataframes\n",
    "COMB = pd.concat([EN,DE],ignore_index = True)\n",
    "COMB = COMB.drop(columns=['In_EN','In_DE'])\n",
    "COMB = COMB.drop_duplicates()\n",
    "\n",
    "#Map values\n",
    "values = set(EN['WikiData ID'])\n",
    "values2 = set(DE['WikiData ID'])\n",
    "COMB['EN'] = COMB['WikiData ID'].isin(values).astype(int)\n",
    "COMB['DE'] = COMB['WikiData ID'].isin(values2).astype(int)\n",
    "\n",
    "#Print Values\n",
    "print(COMB['EN'].value_counts())\n",
    "print(COMB['DE'].value_counts())\n",
    "#display(COMB)\n",
    "\n",
    "#vectorise df columns\n",
    "cs_en = COMB['EN'].values.reshape(1,-1)\n",
    "cs_de = COMB['DE'].values.reshape(1,-1)\n",
    "\n",
    "#get cosine simalarity of links\n",
    "cosine_similarity(cs_en,cs_de)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
